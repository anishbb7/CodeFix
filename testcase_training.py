# -*- coding: utf-8 -*-
"""testcase_training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MLU-NdQAZT4-7dyZ1sCPTsmwSo5CCmm9
"""

from google.colab import drive
drive.mount('/content/drive')

pip install evaluate

!huggingface-cli login

import os
os.environ["HF_TOKEN"] = "hf_token_here"  # <-- replace with your HF token

import torch
torch.cuda.empty_cache()

from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    Trainer,
    TrainingArguments,
    DataCollatorForSeq2Seq,
    TrainerCallback
)
import pandas as pd
import numpy as np
from torch.optim import AdamW
from transformers import get_scheduler
import evaluate
import os
import shutil
import torch

# =========================
# CONFIGURATION
# =========================
model_checkpoint = "Salesforce/codet5p-220m"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

dataset_configs = {
    "testgen": {
        "path": "/content/drive/MyDrive/test_generation_filtered.csv",  # <-- CSV for test case generation
        "input_col": "function",       # <-- column with the program/function
        "output_col": "test_case"      # <-- column with expected test cases
    }
}

# =========================
# LOAD & TOKENIZE FUNCTION
# =========================
def load_and_tokenize(path, input_col, output_col):
    df = pd.read_csv(path)
    if input_col not in df.columns or output_col not in df.columns:
        raise ValueError(f"âŒ Columns {input_col} or {output_col} not found in {path}")

    dataset = load_dataset("csv", data_files=path)

    def tokenize_function(examples):
        model_inputs = tokenizer(
            examples[input_col],
            truncation=True,
            padding="max_length",
            max_length=256
        )
        with tokenizer.as_target_tokenizer():
            labels = tokenizer(
                examples[output_col],
                truncation=True,
                padding="max_length",
                max_length=128
            )
        model_inputs["labels"] = labels["input_ids"]
        return model_inputs

    tokenized_datasets = dataset.map(tokenize_function, batched=True)
    return tokenized_datasets


# =========================
# CALLBACK FOR TRAINING ACCURACY
# =========================
class AccuracyCallback(TrainerCallback):
    def __init__(self, trainer, logging_steps=50):
        self.trainer = trainer
        self.logging_steps = logging_steps
        self.global_step = 0

    def on_step_end(self, args, state, control, **kwargs):
        self.global_step = state.global_step
        if self.global_step % self.logging_steps == 0 and self.global_step > 0:
            self.compute_running_accuracy()

    def compute_running_accuracy(self):
        model = self.trainer.model
        model.eval()
        dataloader = self.trainer.get_train_dataloader()

        correct, total = 0, 0
        with torch.no_grad():
            for i, batch in enumerate(dataloader):
                if i >= 5:
                    break
                batch = {k: v.to(model.device) for k, v in batch.items()}
                outputs = model(**batch)
                preds = outputs.logits.argmax(dim=-1)
                labels = batch["labels"]

                preds = preds.cpu().numpy().flatten()
                labels = labels.cpu().numpy().flatten()

                for p, l in zip(preds, labels):
                    if l != -100:
                        total += 1
                        if p == l:
                            correct += 1

        acc = correct / total if total > 0 else 0.0
        print(f"Step {self.global_step} - Running Accuracy: {acc:.4f}")
        model.train()


# =========================
# SAFE FINAL TRAINING ACCURACY
# =========================
def compute_final_train_accuracy(trainer, model):
    model.eval()
    correct, total = 0, 0
    for i, batch in enumerate(trainer.get_train_dataloader()):
        if i >= 50:   # âœ… only check first 50 batches to avoid OOM
            break
        batch = {k: v.to(trainer.model.device) for k, v in batch.items()}
        with torch.no_grad():
            outputs = model(**batch)
        preds = outputs.logits.argmax(dim=-1)
        labels = batch["labels"]

        preds = preds.cpu().numpy().flatten()
        labels = labels.cpu().numpy().flatten()

        for p, l in zip(preds, labels):
            if l != -100:
                total += 1
                if p == l:
                    correct += 1

    return correct / total if total > 0 else 0.0


# =========================
# TRAINING FUNCTION
# =========================
def train_and_save(tokenized_datasets, final_save_dir):
    model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)

    data_collator = DataCollatorForSeq2Seq(
        tokenizer=tokenizer,
        model=model
    )

    # âœ… Save locally first
    local_dir = "/content/testgen_codet5p"
    os.makedirs(local_dir, exist_ok=True)

    training_args = TrainingArguments(
        output_dir=local_dir,
        overwrite_output_dir=True,
        save_strategy="epoch",
        learning_rate=5e-5,
        per_device_train_batch_size=1,   # small batch
        gradient_accumulation_steps=16,  # effective batch size
        num_train_epochs=1,
        weight_decay=0.01,
        logging_dir=f"{local_dir}/logs",
        logging_steps=50,
        save_total_limit=1,
        fp16=True,
        gradient_checkpointing=True,
        report_to="none"
    )

    optimizer = AdamW(model.parameters(), lr=training_args.learning_rate, weight_decay=training_args.weight_decay)
    num_training_steps = len(tokenized_datasets["train"]) // training_args.per_device_train_batch_size * training_args.num_train_epochs
    lr_scheduler = get_scheduler(
        name="linear",
        optimizer=optimizer,
        num_warmup_steps=100,
        num_training_steps=num_training_steps
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_datasets["train"],
        eval_dataset=None,  # no eval
        tokenizer=tokenizer,
        data_collator=data_collator,
        optimizers=(optimizer, lr_scheduler)
    )

    # Attach accuracy callback
    trainer.add_callback(AccuracyCallback(trainer, logging_steps=50))

    # Train
    train_result = trainer.train()

    # Final accuracy
    print("\nðŸ“Š Calculating final training accuracy...")
    train_acc = compute_final_train_accuracy(trainer, model)

    print(f"\nâœ… Training Complete")
    print(f"   Final Training Loss     : {train_result.training_loss:.4f}")
    print(f"   Final Training Accuracy : {train_acc:.4f}")

    # Save locally
    model.save_pretrained(local_dir)
    tokenizer.save_pretrained(local_dir)

    # âœ… Copy to Drive
    if os.path.exists(final_save_dir):
        shutil.rmtree(final_save_dir)
    shutil.copytree(local_dir, final_save_dir)

    print(f"\nðŸ“‚ Model saved at {final_save_dir}")


# =========================
# RUN TRAINING
# =========================
print("\n===== Training Test Case Generation Model with CodeT5+ =====")
cfg = dataset_configs["testgen"]
dataset_testgen = load_and_tokenize(cfg["path"], cfg["input_col"], cfg["output_col"])
train_and_save(dataset_testgen, "/content/drive/MyDrive/models/testgen_codet5p")