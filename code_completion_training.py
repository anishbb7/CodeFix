# -*- coding: utf-8 -*-
"""Code_completion_training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cEFcolZJzXDi8NLVhcpYmx9rvNjHBP3I
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install datasets accelerate sacrebleu

pip uninstall -y sentence-transformers transformers accelerate

pip install transformers==4.35.2 accelerate==0.25.0

import torch, gc
gc.collect()
torch.cuda.empty_cache()

# ============================================
#   CodeGen Autocomplete Training Script
# ============================================

import os
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer, AutoModelForCausalLM, get_linear_schedule_with_warmup

# ---------------------------------------------------------
# CONFIG
# ---------------------------------------------------------
MODEL_NAME = "Salesforce/codegen-350M-mono"   # CodeGen causal LM
CSV_PATH = "/content/drive/MyDrive/CodeFix_Datasets/autocode_completion.csv"
OUT_DIR = "/content/drive/MyDrive/models/autocode_codegen"
os.makedirs(OUT_DIR, exist_ok=True)

MAX_SEQ_LEN = 2048   # CodeGen maximum
SRC_LIMIT = 1536     # input truncation limit
TGT_LIMIT = 512      # output truncation limit

BATCH_SIZE = 1
EPOCHS = 10
LR = 2e-5
WEIGHT_DECAY = 0.01
GRAD_ACCUM_STEPS = 8
WARMUP_RATIO = 0.1

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Training on:", DEVICE)

# ---------------------------------------------------------
# LOAD DATA
# ---------------------------------------------------------
df = pd.read_csv(CSV_PATH)
df = df.dropna(subset=["input", "output"])
df = df[(df["input"].str.strip() != "") & (df["output"].str.strip() != "")].reset_index(drop=True)

# 80/20 split
train_df, test_df = train_test_split(df, test_size=0.20, random_state=42)

# Further split train into train/val (90/10 of train)
train_df, val_df = train_test_split(train_df, test_size=0.10, random_state=42)

print(f"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}")

# ---------------------------------------------------------
# TOKENIZER & MODEL
# ---------------------------------------------------------
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)
model.gradient_checkpointing_enable()
model.to(DEVICE)

# ---------------------------------------------------------
# DATASET FOR CODEGEN (DECODER-ONLY)
# ---------------------------------------------------------
class CodeGenDataset(Dataset):
    def __init__(self, frame):
        self.inputs = frame["input"].tolist()
        self.outputs = frame["output"].tolist()

    def __len__(self):
        return len(self.inputs)

    def __getitem__(self, idx):
        src = self.inputs[idx]
        tgt = self.outputs[idx]

        # Pre-tokenize input and output
        src_ids = tokenizer(src, truncation=True, max_length=SRC_LIMIT).input_ids
        sep_ids = tokenizer("\n// completion:\n").input_ids
        tgt_ids = tokenizer(tgt, truncation=True, max_length=TGT_LIMIT).input_ids

        # Combine sequences
        combined_ids = src_ids + sep_ids + tgt_ids

        # Proper tokenization for decoder-only LM (CodeGen)
        tokenized = tokenizer.prepare_for_model(
            combined_ids,
            max_length=MAX_SEQ_LEN,
            truncation=True,
            padding="max_length",
            return_tensors="pt"
        )

        input_ids = tokenized["input_ids"].squeeze()
        attn = tokenized["attention_mask"].squeeze()

        # Create labels (mask input part)
        labels = input_ids.clone()
        labels[:len(src_ids)] = -100

        return {
            "input_ids": input_ids,
            "attention_mask": attn,
            "labels": labels
        }


# ---------------------------------------------------------
# DATA LOADERS
# ---------------------------------------------------------
train_ds = CodeGenDataset(train_df)
val_ds = CodeGenDataset(val_df)
test_ds = CodeGenDataset(test_df)

train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)
val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False)
test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)

# ---------------------------------------------------------
# OPTIMIZER & SCHEDULER
# ---------------------------------------------------------
optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)
steps_per_epoch = (len(train_loader) + GRAD_ACCUM_STEPS - 1) // GRAD_ACCUM_STEPS
t_total = EPOCHS * steps_per_epoch
num_warmup = int(WARMUP_RATIO * t_total)

scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup, t_total)

scaler = torch.cuda.amp.GradScaler()

# ---------------------------------------------------------
# TRAINING LOOP
# ---------------------------------------------------------
import matplotlib.pyplot as plt

epoch_losses = []   # store loss for graph

global_step = 0

for epoch in range(EPOCHS):
    print(f"\n----- Epoch {epoch+1}/{EPOCHS} -----")
    model.train()
    running_loss = 0.0
    optimizer.zero_grad(set_to_none=True)

    for step, batch in enumerate(train_loader, 1):
        batch = {k: v.to(DEVICE) for k, v in batch.items()}

        with torch.cuda.amp.autocast():
            outputs = model(**batch)
            loss = outputs.loss / GRAD_ACCUM_STEPS

        scaler.scale(loss).backward()
        running_loss += loss.item() * GRAD_ACCUM_STEPS

        if step % GRAD_ACCUM_STEPS == 0:
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad(set_to_none=True)
            scheduler.step()
            global_step += 1

        if step % (GRAD_ACCUM_STEPS * 20) == 0:
            print(f"Step {step}: Loss {running_loss/step:.4f}")

    # Average epoch loss
    epoch_loss = running_loss / len(train_loader)
    print(f"Epoch {epoch+1} avg loss: {epoch_loss:.4f}")

    epoch_losses.append(epoch_loss)


# ---------------------------------------------------------
# SAVE MODEL
# ---------------------------------------------------------
save_dir = os.path.join(OUT_DIR, "codegen_autocomplete_finetuned")
model.save_pretrained(save_dir)
tokenizer.save_pretrained(save_dir)

print("Model saved to:", save_dir)

plt.figure(figsize=(8,5))
plt.plot(range(1, EPOCHS+1), epoch_losses, marker='o', linewidth=2)
plt.title("Training Loss per Epoch")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.show()

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# ---------------------------------------------------
# PATH TO YOUR FINE-TUNED MODEL
# ---------------------------------------------------
MODEL_PATH = "/content/drive/MyDrive/models/autocode_codegen/codegen_autocomplete_finetuned"

# ---------------------------------------------------
# LOAD MODEL + TOKENIZER
# ---------------------------------------------------
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
model = AutoModelForCausalLM.from_pretrained(MODEL_PATH)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
model.eval()

print("Loaded model from:", MODEL_PATH)

# ---------------------------------------------------
# PREDICT FUNCTION (AUTOCOMPLETE)
# ---------------------------------------------------
def predict_autocomplete(code_prefix, max_new_tokens=200):

    # FORCE the model to interpret this as METHOD COMPLETION
    guide = (
        "/* Complete the missing Java code inside the existing method only. */\n"
    )

    prompt = guide + code_prefix + "\n// completion:\n"

    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048).to(device)

    with torch.no_grad():
        output_ids = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            top_p=0.92,
            temperature=0.3,
            eos_token_id=tokenizer.eos_token_id
        )

    text = tokenizer.decode(output_ids[0], skip_special_tokens=True)

    return text.split("// completion:")[-1].strip()

incomplete_code = """
public class Test {
    public static int[] sum(int[] a) {
        // TODO
"""

print(predict_autocomplete(incomplete_code))



#old code
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling
import pandas as pd
import numpy as np

# =========================
# CONFIGURATION
# =========================
model_checkpoint = "Salesforce/codegen-350M-multi"  # You can change this
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
tokenizer.pad_token = tokenizer.eos_token  # For causal LM

# Dataset paths and column names
dataset_configs = {
    "completion": {
        "path": "/content/drive/MyDrive/code_completion.csv",
        "input_col": "input_set",
        "output_col": "output_set"
    }
}

# =========================
# LOAD & TOKENIZE FUNCTION
# =========================
def load_and_tokenize(path, input_col, output_col):
    # Read CSV
    df = pd.read_csv(path)
    if input_col not in df.columns or output_col not in df.columns:
        raise ValueError(f"âŒ Columns {input_col} or {output_col} not found in {path}")

    # Merge input and output into one "text" field for LM training
    df["text"] = df[input_col].astype(str) + " " + df[output_col].astype(str)

    # Save temp CSV with only text column (datasets lib needs it)
    temp_path = "/content/drive/MyDrive/temp_dataset.csv"
    df[["text"]].to_csv(temp_path, index=False)

    # Load dataset
    dataset = load_dataset("csv", data_files=temp_path)["train"]

    # ðŸ”¹ Internal split: 90% train, 10% test
    dataset = dataset.train_test_split(test_size=0.2, seed=42)

    # Tokenization
    def tokenize_function(examples):
        return tokenizer(examples["text"], truncation=True, padding="max_length", max_length=256)

    tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=["text"])
    return tokenized_datasets

# =========================
# TRAINING FUNCTION
# =========================
def train_and_save(tokenized_datasets, save_dir):
    model = AutoModelForCausalLM.from_pretrained(model_checkpoint)

    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False
    )

    training_args = TrainingArguments(
        output_dir=save_dir,
        overwrite_output_dir=True,
        save_strategy="epoch",
        learning_rate=5e-5,
        per_device_train_batch_size=4,
        per_device_eval_batch_size=4,
        num_train_epochs=2,
        weight_decay=0.01,
        gradient_accumulation_steps=4,
        logging_dir=f"{save_dir}/logs",
        logging_steps=50,
        save_total_limit=1,
        fp16=True
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_datasets["train"],   # âœ… Train on split
        eval_dataset=tokenized_datasets["test"],     # âœ… Eval on split
        tokenizer=tokenizer,
        data_collator=data_collator,
    )

    trainer.train()

    # Save model
    model.save_pretrained(save_dir)
    tokenizer.save_pretrained(save_dir)
    print(f"âœ… Model saved at {save_dir}")

# =========================
# TRAIN EACH MODEL
# =========================
print("\n===== Training Auto Code Completion Model =====")
cfg = dataset_configs["completion"]
dataset_completion = load_and_tokenize(cfg["path"], cfg["input_col"], cfg["output_col"])
train_and_save(dataset_completion, "/content/drive/MyDrive/models/code_completion_model")

results = trainer.evaluate()
print(f"âœ… Final Accuracy: {results['eval_accuracy']:.4f}")

from transformers import AutoTokenizer, AutoModelForCausalLM

# Load the model and tokenizer
model_name = "/content/drive/MyDrive/models/code_completion_model"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Prompt for incomplete Java function
java_prompt = """public static int divide(int a, int b) {
       """

# Function to trim after full function ends
def trim_after_function(decoded: str) -> str:
    brace_count = 0
    trimmed_code = []
    inside_function = False

    for line in decoded.splitlines():
        trimmed_code.append(line)

        if '{' in line:
            brace_count += line.count('{')
            inside_function = True

        if '}' in line:
            brace_count -= line.count('}')
            if inside_function and brace_count == 0:
                break  # Function end reached

    return "\n".join(trimmed_code).strip()

# Tokenize and generate
inputs = tokenizer(java_prompt, return_tensors="pt").to(model.device)
outputs = model.generate(
    **inputs,
    max_length=350,
    do_sample=True,
    temperature=0.7,
    pad_token_id=tokenizer.eos_token_id
)

# Decode and clean
decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
decoded = trim_after_function(decoded)

print(decoded)
