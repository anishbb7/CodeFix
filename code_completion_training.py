# -*- coding: utf-8 -*-
"""Code_completion_training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15tmEhV_a45Fe2hqGvlrez-Qty2MoHPq9
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install transformers datasets accelerate

import torch, gc
gc.collect()
torch.cuda.empty_cache()

from datasets import load_dataset

#no need to run
dataset = load_dataset('csv', data_files={
    'train': '/content/drive/MyDrive/code_completion.csv',
    'validation': '/content/drive/MyDrive/code_completion.csv'
})
print(dataset)
print(dataset['train'].column_names)

#no need to run
dataset = load_dataset('csv', data_files={
    'train': '/content/drive/MyDrive/debugging.csv',
    'validation': '/content/drive/MyDrive/debugging.csv'
})
print(dataset)
print(dataset['train'].column_names)

#no need to run
dataset = load_dataset('csv', data_files={
    'train': '/content/drive/MyDrive/test_generation.csv',
    'validation': '/content/drive/MyDrive/test_generation.csv'
})
print(dataset)
print(dataset['train'].column_names)

from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling
import pandas as pd

# =========================
# CONFIGURATION
# =========================
model_checkpoint = "Salesforce/codegen-350M-multi"  # You can change this
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
tokenizer.pad_token = tokenizer.eos_token  # For causal LM

# Dataset paths and column names
dataset_configs = {
    "completion": {
        "path": "/content/drive/MyDrive/code_completion.csv",
        "input_col": "input_set",
        "output_col": "output_set"
    },
    "bugfix": {
        "path": "/content/drive/MyDrive/debugging.csv",
        "input_col": "buggy_code",
        "output_col": "corrected_code"
    },
    "testcase": {
        "path": "/content/drive/MyDrive/test_generation.csv",
        "input_col": "function",
        "output_col": "test_case"
    }
}

# =========================
# LOAD & TOKENIZE FUNCTION
# =========================
def load_and_tokenize(path, input_col, output_col):
    # Read CSV
    df = pd.read_csv(path)
    if input_col not in df.columns or output_col not in df.columns:
        raise ValueError(f"❌ Columns {input_col} or {output_col} not found in {path}")

    # Merge input and output into one "text" field for LM training
    df["text"] = df[input_col].astype(str) + " " + df[output_col].astype(str)

    # Save temp CSV with only text column (datasets lib needs it)
    temp_path = "/content/drive/MyDrive/temp_dataset.csv"
    df[["text"]].to_csv(temp_path, index=False)

    # Load into HuggingFace Dataset
    dataset = load_dataset("csv", data_files=temp_path)

    # Tokenization
    def tokenize_function(examples):
        return tokenizer(examples["text"], truncation=True, padding="max_length", max_length=256)

    tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=["text"])
    return tokenized_datasets

# =========================
# TRAINING FUNCTION
# =========================
def train_and_save(tokenized_datasets, save_dir):
    model = AutoModelForCausalLM.from_pretrained(model_checkpoint)

    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False
    )

    training_args = TrainingArguments(
        output_dir=save_dir,
        overwrite_output_dir=True,
        save_strategy="epoch",
        learning_rate=5e-5,
        per_device_train_batch_size=4,
        per_device_eval_batch_size=4,
        num_train_epochs=1,
        weight_decay=0.01,
        gradient_accumulation_steps=4,
        logging_dir=f"{save_dir}/logs",
        logging_steps=50,
        save_total_limit=2,
        fp16=True
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_datasets["train"],
        eval_dataset=tokenized_datasets["train"],  # Using same dataset for eval here
        tokenizer=tokenizer,
        data_collator=data_collator,
    )

    trainer.train()
    model.save_pretrained(save_dir)
    tokenizer.save_pretrained(save_dir)
    print(f"✅ Model saved at {save_dir}")

# =========================
# TRAIN EACH MODEL
# =========================
print("\n===== Training Auto Code Completion Model =====")
cfg = dataset_configs["completion"]
dataset_completion = load_and_tokenize(cfg["path"], cfg["input_col"], cfg["output_col"])
train_and_save(dataset_completion, "/content/drive/MyDrive/models/code_completion_model")

from transformers import AutoTokenizer, AutoModelForCausalLM

# Load the model and tokenizer
model_name = "/content/drive/MyDrive/models/code_completion_model"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Prompt for incomplete Java function
java_prompt = """public class MatrixPrint {
    public static void main(String[] args) {
        // Define a 2x2 matrix
        """

# Function to trim after full function ends
def trim_after_function(decoded: str) -> str:
    brace_count = 0
    trimmed_code = []
    inside_function = False

    for line in decoded.splitlines():
        trimmed_code.append(line)

        if '{' in line:
            brace_count += line.count('{')
            inside_function = True

        if '}' in line:
            brace_count -= line.count('}')
            if inside_function and brace_count == 0:
                break  # Function end reached

    return "\n".join(trimmed_code).strip()

# Tokenize and generate
inputs = tokenizer(java_prompt, return_tensors="pt").to(model.device)
outputs = model.generate(
    **inputs,
    max_length=350,
    do_sample=True,
    temperature=0.7,
    pad_token_id=tokenizer.eos_token_id
)

# Decode and clean
decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
decoded = trim_after_function(decoded)

print(decoded)